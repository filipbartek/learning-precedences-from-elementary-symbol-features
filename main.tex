\input{preamble}

\title{Learning Precedences from Simple Symbol Features\thanks{Supported by the ERC Consolidator grant AI4REASON no. 649043 under the EU-H2020 programme and the Czech Science Foundataion project 20-06390Y.}}
\titlerunning{Learning Precedences from Elementary Symbol Features}
\author{Filip B\'{a}rtek \and Martin Suda}
\authorrunning{B\'{a}rtek, Suda}
\institute{Czech Technical University in Prague, Czech Republic}

\begin{document}

\maketitle

\begin{abstract}
A simplification ordering, typically specified by a symbol precedence,
is one of the key parameters of the superposition calculus, contributing
to shaping the search space navigated by a saturation-based \acrlong*{atp}.
Thus the choice of a precedence can have a great impact on the prover's performance.
In this work, we design a system for proposing symbol precedences
that should lead to solving a problem quickly.
The system relies on machine learning to extract this information from
past successful and unsuccessful runs of a theorem prover over a set of problems and randomly sampled precedences.
It uses a small set of simple human-engineered symbol features as the sole
basis for discriminating the symbols. This allows for a direct comparison
with precedence generation schemes designed by prover developers.
% \todo[author=Martin]{Let's add this when we have the results finalized:
% Training a linear model converges to ordering the symbols by the number of their occurrences.}
\end{abstract}

\section{Introduction}

Modern saturation-based \glspl*{atp} such as E \cite{SCV:CADE-2019}, SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09} 
or \gls*{vampire} \cite{Kovacs2013}
use the superposition calculus \cite{Nieuwenhuis2001} as their underlying inference system.
Superposition is built around the paramodulation inference \cite{Robinson1983} crucially
constrained by simplification ordering on terms and literals, which is supplied as a parameter of the calculus.
Both of the two main classes of simplification orderings used in practice,
i.e., the \acrlong*{kbo} \cite{Knuth1983}
and the \acrlong*{lpo} \cite{Kamin1980},
are mainly determined by a %pair of 
\emph{symbol precedence}, a (partial) ordering on the signature symbols.\footnote{
KBO is further parameterized by symbol weights, but our reference implementation in Vampire~\cite{Kovacs2013} 
uses for efficiency reasons only weights equal to one \cite{DBLP:conf/cade/KovacsMV11} and so we do not consider this parameter here.}

While the superposition calculus is known \cite{DBLP:journals/logcom/BachmairG94} to be refutationally complete for any simplification ordering, the choice of the precedence may have a significant impact on how long it takes to solve a given problem.
In a well-known example, prioritizing in the precedence the predicates introduced during the Tseitin transformation of an input formula \cite{Tseitin1983} exposes the corresponding literals to resolution inference during early stages of the proof search,
with the effect of essentially undoing the transformation and thus threatening with an exponential blow-up
the transformation is designed to prevent \cite{Reger2016}.
%
ATPs typically offer a few heuristic schemes for generating the symbol precedences.
For example, the successful \texttt{invfreq} scheme in E \cite{E-manual} orders the symbols by the number of occurrences in the input problem,
prioritizing symbols that occur the least often for early inferences.
Experiments with random precedences have shown that the existing schemes often fail to come close to the optimum precedence \cite{RegerSuda2017}, revealing there is a large potential for further improvements.

In this work, we design a system that, when presented with a First-Order Logic (FOL) problem,
proposes a symbol precedence that will likely lead to solving the problem quickly.
The system relies on the techniques of supervised machine learning and extracts
such theorem-proving knowledge from successful (and unsuccessful) runs of 
the Vampire theorem prover~\cite{Kovacs2013} when run over a variety of FOL problems equipped
with randomly sampled symbol precedences. 
We assume that by learning to solve already solvable problems quickly,
the acquired knowledge will generalize and help solving problems previously out of reach.
% This general assumption is shared with other projects for automatically learning theorem
% proving strategies from previous experience, such as the MaLeS system \cite{DBLP:journals/jar/KuhlweinU15}.
As a first step in a more ambitious project,
we focus here on representing the symbols in a problem by a fixed set of simple human-engineered features
(such as the number the occurrences used by \texttt{invfreq} scheme mentioned above)\footnote{Automatic
feature extraction using neural-networks is planned for future work.}
and, to simplify the experimental setup, we restrict our attention to learning precedences for predicate symbols only.\footnote{
Our theoretical considerations, however, apply equally to learning function symbol precedences.}

Learning to predict good precedences poses several interesting challenges that we address in this work.
First, it is not immediately clear how to characterise a precedence, a permutation of a finite set of symbols,
by a real-valued feature vector to serve as an input for a learning algorithm. 
Additionally, to be able to generalise across problems
we need to do it in a way which does not presuppose a fixed signature size. 
There is also a complication, when sampling different problems,
that some problems may be easy to solve for almost every precedence and others hard.
In theorem proving, running times typically vary considerably.
Finally, even with a regression model ready to predict the prover's performance 
under a particular precedence $\pi$, we still need to solve the task of finding 
the ideally optimal precedence $\pi^*$ according to this model,
which cannot be simply solved by enumerating all the permutations and running the prediction for each
due to their huge number.

Our way of addressing the above sketched challenges lies in using \emph{pairwise symbol preferences}
to characterise a precedence, normalising the target prover run times on per problem basis,
and in the use of ``second-order'' learning of the preferences for symbols abstracted by their features.
These concepts are introduced in \Cref{sect:overview} and later formalized in \Cref{sec:architecture}.
\Cref{sec:evaluation} presents the results of our experimental evaluation of the proposed technique over the TPTP \cite{Sut17} benchmark.
We start our exposition by fixing the notation and basic concepts in \Cref{sect:prelim}.

% Final paragraph, with forward reference, explains how we tackle the challenges and where it is explained
% 1) preference values
% 2) normalization
% 3) learning to order things

\section{Preliminaries} \label{sect:prelim}

% \subsection{Terminology}

We assume that the reader is familiar with basic concepts used in \gls*{fol} theorem proving.
We use this section to recall and formalize the key notions relevant for our work.

% such as \gls{fol} problem, problem signature, predicate symbol, function symbol, clause and \gls{cnf}.
% \todo[author=Filip]{Do we really need to assume this familiarity? We explain the meaning of "problem" below.}
% Furthermore we assume familiarity with basics of the inner workings of saturation-based \glspl{atp},
% including clausification and saturation loop.
% \todo[author=Filip]{Cite some textbook.}

% Regression algorithms; (Maybe not here yet: we do it in Python - scikitlearn, numpy, \ldots)

\paragraph{Problem}
A \emph{(first-order) problem} is a pair \(P = (\symbols, \clauses)\),
where \(\symbols = (s_1, s_2, \ldots, s_n)\) is a list of (predicate and function) symbols called the \emph{signature},
and \(\clauses\) is a set of first-order
clauses built over the symbols of \(\symbols\).
% This is where we assume familiarity with FOL.

The problem is either given directly by the user or 
could be the result of clausifying a general \gls*{fol} formula \(\varphi\),
in which case we know which of the symbols were introduced during the clausification
(namely during Tseitin transformation and skolemization; see e.g. \citet{DBLP:books/el/RV01/NonnengartW01})
and which occurred in the conjecture (if it was present).
% \todo[author=Filip]{Mention that we also know which clauses form the conjecture. We use this in the feature \texttt{inGoal}.}

\paragraph{Precedence}
Given a problem \(P = (\symbols, \clauses)\) with \(\symbols = (s_1, s_2, \ldots, s_n)\),
a precedence \(\pi_P\) is a permutation, i.e.~a bijective mapping, of the set of indices \(\{1,\ldots,n\}\).
A precedence \(\pi_P\) \emph{determines} a (total) ordering on \(\symbols\) as follows:
% The word "determines" is emphasized because it is being defined.
\(s_{\pi_P(1)} < s_{\pi_P(2)} < \ldots < s_{\pi_P(n)}.\)

\paragraph{Simplification Orderings} are orderings on terms used to parameterize the superposition calculus \cite{Nieuwenhuis2001}
employed by modern saturation-based theorem provers. The two classes of simplification orderings most commonly used in practice,
% \todo[author=Filip]{Why do we believe they are most commonly used in practice? Can we cite some source?}
the \acrlongpl*{kbo} \cite{Knuth1983} and the \acrlongpl*{lpo} \cite{Kamin1980}, are both defined in terms 
of a user-supplied (possibly partial) ordering $<$ %(typically also called the precedence) 
on the given problem's signature \(\symbols\).
In this work, we assume that the theorem prover uses a simplification ordering from one of these two classes
relying on the ordering on \(\symbols\) determined by a precedence \(\pi_P\) to construct such a simplification ordering.

\paragraph{Performance measure} A saturation-based \gls*{atp} \emph{solves} a problem \(P = (\symbols,\clauses)\)
(under a particular fixed strategy and a determined symbol precedence \(\pi_P\))  by either
\begin{itemize}
\item
	deriving from \(\clauses\) a contradiction in the form of the empty clause,
	in which case \(P\) is shown \emph{unsatisfiable}, or
\item
	finitely saturating the set of clauses \(\clauses\) without deriving the contradiction,
	in which case \(P\) is shown \emph{satisfiable}.\footnote{We assume a \emph{refutationally complete} calculus and saturation strategy.}	
\end{itemize}
In both cases, we take the number of iterations of the employed saturation algorithm (see, e.g., \citet{DBLP:journals/jsc/RiazanovV03} for an overview) 
as a measure of the effort that the \gls*{atp} took to solve the problem.
We refer to this measure as the \emph{\gls*{abstract-solving-time}} and denote it \(\AbstractTime(P,\pi_P)\).\footnote{The advantage of using abstract solving time is that it does not depend on the hardware used for the computation.}

In practice, an \gls*{atp} can also run out of resources, typically out of the allocated time.
In that case, the abstract solving time is \emph{undefined}: \(\AbstractTime(P,\pi_P) = \bot\).
%
While it may happen that running an \gls*{atp} with the same problem and symbol precedence two times
yields a different result each time (namely succeeding one time and failing another time),
such cases are rare and we ensure they do not interfere with the learning process by caching the results.


\paragraph{\Gls*{order-matrix}}
Given a permutation \(\pi\) of the set of indices \(\{1,\ldots,n\}\),
the \gls*{order-matrix} \(\OrderMatrix(\pi)\) is a binary matrix of size \(n \times n\)
defined in the following manner:
\begin{align*}
\OrderMatrix(\pi)_{i, j} = \iverson{\inv{\pi}(i) < \inv{\pi}(j)},
\end{align*}
where we use \(\iverson{P}\) to denote the Iverson bracket \cite{Iverson1962} applied to a proposition \(P\),
% In \citet{Iverson1962}, the expression is called "relational statement".
evaluating to 1 if \(P\) is true, and 0 otherwise.
In other words, for a symbol precedence \(\pi_P\), \(O(\pi_P)_{i, j} = 1\) if
the precedence \(\pi_P\) orders the symbol \(s_i\) before the symbol \(s_j\),
and \(O(\pi_P)_{i, j} = 0\) otherwise.

\paragraph{Flattened matrix}
Given a matrix \(M\) of size \(n \times n\),
\(\flatten{M}\) is the vector of length \(n^2\) obtained by flattening \(M\):
\begin{align*}
\flatten{M}_{(i - 1) n + j} = M_{i, j}
\end{align*}
for every \(i,j\in\{1,\ldots,n\}\). For our use the exact way of mapping the 
matrix elements to the vector indices is not important. We mostly just need a vector representation
of the data contained in the given matrix to have access to the dot product operation. % which is more natural for vectors.

\paragraph{Linear regression}
is an approach to modeling the relationship between scalar \emph{target} values $y_i \in \re$
and one or more \emph{input} variables $\mathbf{x}_i = (x^1_i,\ldots,x^k_i)$, $i = 1,\ldots,n$.
The relationship is modeled using a linear predictor function:
\[\hat{y}_i =  \mathbf{x}_i \cdot \mathbf{w} + b, \]
whose unknown model parameters $\mathbf{w}\in \re^k$ and $b \in \re$ are estimated from the data.
We call the vector $\mathbf{w}$ the \emph{coefficients} of the model and $b$ the \emph{intercept}.
Most commonly, the parameters are picked to minimize the so called mean squared error:
\[\mathit{MSE} = \frac{1}{n} \sum\limits_{i=1}^n (\hat{y}_i - y_i)^2, \]
but other norms are also possible \cite{hastie01statisticallearning}.

% \todo[inline,author=Martin]{Define sufficient apparatus to be able to prove that if we use linear regression for the general preference prediction, than we can construct optimum precedence in linear time from the coefficients. Write down the proof.}

\paragraph{Basic assumptions}

For the discussion that follows,
we assume a fixed %\gls{fol} 
\gls*{atp} that uses the superposition calculus with a simplification ordering parameterized by a symbol precedence.
%\todo[author=Filip]{Consider different approach: "For the discussion that follows,
%we assume that Vampire is used. We keep the discussion general enough to accommodate any precedence-parameterized ATP."
%and use "Vampire" instead of "ATP" in sections that follow.}
While the practical experiments described in \Cref{sec:evaluation} use the \gls*{atp} \gls*{vampire} \cite{Kovacs2013},
% \todo[author=Filip]{Remove if we end up omitting practical experiments.}
the model architecture does not assume a particular \gls*{atp}
and is compatible with any superposition-based \gls*{atp} such as E \cite{SCV:CADE-2019}, SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09} or  \gls*{vampire}.
% 
Within the prover a particular saturation strategy is fixed including a time limit.
If the prover runs out of time before solving a problem, we record \(\AbstractTime(P,\pi_P) = \bot\).

%The parameters may specify for example a literal comparison mode, a saturation algorithm
%\todo[author=Filip]{Are these examples or their terminology too specific for Vampire? Can we expect the reader to understand them?}

% We specifically assume that a finite time limit is used
% because some proof searches may take too long for practical use\cite{?}
% and we need to handle such searches gracefully.
% Note that an ATP that is complete is guaranteed to halt on a solvable problem.
% However, the solving time may vary greatly across the precedences.

% \todo[inline,author=Filip]{Consider: Mention that we need to call the configured ATP to clausify 
% the problem so that we know all the symbols including Tseitin and Skolem.}

\section{General considerations and overview} \label{sect:overview}

The aim of this work is to design a system that learns to suggest good symbol precedences to an ATP 
from observations of the ATP's performance on a class \(\mathcal{P}\) of problems with randomly sampled precedences. 
Given a problem \(P = (\symbols,\clauses)\) with \(|\symbols|=n\), we consider a precedence \(\pi_P\) good
if it leads to a low \(\AbstractTime(P,\pi_P)\)
among the $n!$ possible precedences for \(P\).
% ze n! je super large -> almost infinite source of data (for non-trivial n)
Note that for problems with a signature with more than a few symbols, repeatedly running the prover 
with random precedences represents an effectively infinite source of training data.

Ideally, we would like to learn general theorem proving knowledge, not too dependent on $\mathcal{P}$,
which could be later explained and compared to precedence generation schemes manually designed 
by the prover developers. Let us quickly recall one such scheme, already mentioned in the introduction,
called \texttt{invfreq} in E \cite{SCV:CADE-2019}. The prover's manual \cite{E-manual} explains:
\begin{quote}
Sort symbols by frequency (frequently occurring symbols are smaller).
\end{quote}
What is common to basically all manually designed schemes, is that they pick a certain scalar property of symbols 
% information retrieval terminology
(here it is symbol the frequency, i.e.~the number of occurrences of the symbol in the given problem)
and obtain a precedence by \emph{sorting} the symbols using that property.

\paragraph{Decomposing}

We might want our system to also learn a certain property of symbols and use sorting to generate and suggest a precedence. 
However, it is not clear how to ``extract'' such property from the observed data, since we only have 
access to the target values for full precedences. Our idea for ``decomposing'' these values into pieces 
that somehow relate to individual symbols (and can thus be ``transferred'' across problems) is to take a detour using symbol pairs:
we assume that the performance of the ATP on \(P\) given \(\pi_P\), i.e.~our measure $\AbstractTime(P,\pi_P)$, can be predicted 
from a sum of individual contributions corresponding to facts of the form 
\[\pi_P\text{ orders the symbol }s_i\text{ before the symbol }s_j.\]
This is in line with how a prover developer could reason about a precedence generating scheme:
Even when it is not clear how good or bad a symbol is in absolute terms,
one might have an intuition that a symbol from a certain class should preferably 
come before a symbol from another class in the precedence  
(e.g., symbols introduced during clausification should typically be smaller than others)
and assign some weight to this piece of intuition. 

In~\Cref{sect:preferences} we formalize this idea using the notion of \emph{preference matrix}
and show how, for each problem in isolation, such preference matrix can be learned using linear regression.

\paragraph{Learning across problems}
Symbol preferences learned on a particular problem are inherently tied to that problem
and do not immediately carry over to other problems. The main reason for this 
is that symbols themselves only appear in the context of a particular problem.\footnote{
On certain benchmarks, such as those coming from translations of mathematical libraries \cite{KaliszykU13b},
symbols maintain identity and meaning across individual problems. However, since our goal in this work
is to learn general theorem proving knowledge, we do not use the assumption of aligned signatures.}
That is why we resort to representing symbols by their \emph{features} (cf.~\Cref{sect:embeddings})
when aggregating the learned preferences across different problems.
This is in more details explained further below and, formally, in \Cref{sec:general-preference}.

We also strive to ensure that the preference values across problems have possibly the same magnitude.
Note that \(\AbstractTime(P,\pi_P)\) may vary a lot for a fixed problem \(P\) but all the more so across
problems. To obtain commensurable values, we normalize
%\todo[author=Filip]{I would prefer to use "normalize" since it seems more common to me. Is there an advantage in using "normalise"?}
(see \Cref{sec:precedence-valuation}) the prover performance data
on a per problem basis before learning the preferences. Normalization also deals with
supplying a concrete value to those runs which did not finish, i.e.~have \(\AbstractTime(P,\pi_P) = \bot\).

For the follow-up steps we assume that preferences learned from such normalised data relate to one another
across problems in a meaningful way.

\paragraph{``Second-order'' regression}
Once the symbols are abstracted by their feature vectors, we can collect symbol preferences from all the tested problems
and turn this collection into another regression task. 
Note that at this moment, the preferences, which were obtained as the coefficients learned by linear regression,
themselves become the regression target. Thus, in a certain sense, we now do second-order learning.
It should be stressed though, that while the learning of the preferences \emph{requires} a linear regression model by design,
this second-order regression does not need to be linear and more sophisticated models can be experimented with. 

The details of this step are given in \Cref{sec:general-preference}.

\paragraph{Preference prediction and optimization} Once the second-oder model has been learned, we can predict 
preferences for any pair of symbols based on their feature vectors and thus also predict,
given a problem $P$, how many steps will a prover require to solve it using a particular precedence $\pi_P$.
(For this second step, we reverse the idea of decomposition:
we sum up those predicted preferences that correspond to pairs of symbols $s_i,s_j$
such that $\pi_P$ orders the symbol $s_i$ before the symbol $s_j$ -- see \Cref{sect:construction} for details).

Having access to an estimate of performance for each precedence $\pi_P$, the final step 
is to look for a precedence $\pi^*_P$ that ideally minimises the predicted performance measure 
over all the $n!$ possible precedences on $P$'s signature. Since finding the true
optimum could be computationally hard, we resort to using an approximation algorithm by \citet{Cohen2011}.

The algorithm is recalled in \Cref{sect:ltot}.

% ---> learning to order things (greedy), 
% hillclimb - from greedy, from random (try many times)

% computational complexity considerations

\iffalse

\subsection{Main goals}
\todo[inline,author=Filip]{Remove this section if most of its content has been included in other sections. This section seems unwieldy.}

We strive to create a system that satisfies the following properties:

\begin{itemize}
	\item When presented with an arbitrary \gls{fol} problem,
	the system proposes a symbol precedence for this problem
	that maximizes the expected chance of the problem being solved
	within the allocated time.
	\todo[author=Filip]{What distribution of problems do we optimize for?}
	\todo[inline,author=Martin]{Maybe simply minimize the expected (abstract) solving time (i.e. measured in the number of iterations of the main loop!).}
	
	\item The system is trained on a collection of executions of an \gls{atp}
	on a training set of \gls{fol} problems
	with uniformly random symbol precedences for each training problem.
	
	\item The architecture is general enough to be able to learn
	all of the standard precedence heuristics implemented in Vampire.\cite{?}
	\todo[author=Filip]{Describe in more detail. We care specifically about the model capacity, not the learning convergence.}
\end{itemize}

\todo[inline,author=Filip]{Should we mention the following side goal? "Make it possible to backpropagate gradient to the input features for future GNN training."}

\fi

\section{Architecture}
\label{sec:architecture}

\subsection{Values of precedences}
\label{sec:precedence-valuation}

% Based on the assumption that a short proof search is more likely to succeed in a limited time\cite{?},
% we train the system to prefer precedences that lead to a short proof search.
% This allows us to use easy problems for training, facilitating the collection of training data.
% \todo[author=Filip]{Do we really want to do this? The experiment with uniformly random training problems shows better results than training on easy problems.}

We define the base cost
value \(\CostBase(P, \pi_P)\) of precedence \(\pi_P\) on problem \(P\)
according to the outcome of the proof search configured to use this precedence:

\begin{itemize}
	\item If the proof terminates successfully,
	\(\CostBase(\pi_P)\) is the number of iterations of the saturation loop
	started during the proof search: \(\CostBase(\pi_P) = \AbstractTime(P, \pi_P)\).
	\item If the proof search fails (meaning that \(\AbstractTime(P, \pi_P) = \bot\)),
	then \(\CostBase(\pi_P)\) is the maximum number
	of saturation loop iterations encountered in successful training proof searches on this problem:
	\(\CostBase(\pi_P) = \max_{\pi_P' \in \Pi^+_P} \AbstractTime(P, \pi_P')\),
	where \(\Pi^+_P\) is the set of all training precedences on problem \(P\)
	that yield a successful proof search.
	% \todo[author=Martin,inline]{Not even times 2?}
\end{itemize}
We further normalize the cost values by the following operations:
\begin{enumerate}
	\item Logarithmic scaling:
	For each solvable problem, running proof search with uniformly random predicate precedences
	% \todo[author=Filip]{As far as I remember, I have only examined the distributions with random predicate precedences. Here we would like to reason about general symbol precedences though.}
	reveals a distribution of abstract solving times on successful executions.
	Examining these distributions for various problems suggests that they are usually
	approximately log-normal.
	%\todo[author=Filip]{Justify better.}
	To make further scaling by standardization reasonable,
	% \todo[author=Filip]{What exactly do we mean by "reasonable"?}
	we first transform the base costs by taking their logarithm.
	\item Standardization: %\cite{?}:
	%\todo[author=Filip]{We can use the more general term "normalization" instead.
	% Our goal is to normalize the values and standardization is the means we choose.}
	Independently for each problem,
	we apply an affine transformation so that the resulting cost values
	have the mean 0 and standard deviation 1.
	This ensures that the values are comparable across problems.
	% \todo[author=Filip]{Make less handwavy.}
\end{enumerate}
Let \(\CostStd(\pi_P)\) denote the resulting cost value of permutation \(\pi_P\)
after the scaling and standardization.

\subsection{Problem preference matrix learning}
\label{sect:preferences}

%    Let \(s_i \neq s_j\) be two symbols in a given \gls{problem} \(P\).
%    Each precedence \(\pi_P\) orders these symbols in some order --
%    either \(\inv{\pi_P}(i) < \inv{\pi_P}(j)\) or \(\inv{\pi_P}(i) > \inv{\pi_P}(j)\).
%    Ordering of some pairs of symbols can have a great impact on the speed of the proof search.
%    For example, prioritizing inferences on clauses with predicate symbols
%    introduced in Tseitin transformation
%    may lead to an exponential expansion of the affected clauses.
%    \todo[author=Filip]{Consider omitting this justification because we have mentioned it in section Introduction.}

Given a problem \(P\) with \(n\) symbols,
a \emph{preference matrix} \(W_P\) is any matrix over \(\re\) of size \(n \times n\).
% \todo[author=Filip]{Is it ok to use the name "preference" for a function that is to be minimized, thus actually a cost? I adopted it from LtOT terminology. Alternative: pairwise cost.}
We define the \emph{proxy cost of precedence \(\pi_P\) under preference \(W_P\)}
to be the sum of the preference values \({W_P}_{i, j}\) of all symbol pairs \(s_i, s_j\)
ordered by \(\pi_P\) such that \(s_i\) comes before \(s_j\):
\begin{align*}
\CostProxy(\pi_P, W_P) = \sum_{i, j} \iverson{\inv{\pi_P}(i) < \inv{\pi_P}(j)} {W_P}_{i, j}
= \flatten{\OrderMatrix(\pi_P)} \cdot \flatten{W_P}
\end{align*}
where \(\flatten{\OrderMatrix(\pi_P)} \cdot \flatten{W_P}\) is the dot product
of the flattened matrices \(\OrderMatrix(\pi_P)\) and \(W_P\).
% \todo[author=Filip]{Is it not too confusing to have two expressions?}
% \todo[author=Filip]{Shall we call it "surrogate loss"? Our proxy cost seems to be positively aligned with the conventional use of the term but typically "surrogate loss" is differentiable.}

For any given problem we can uniformly sample precedences \(\pi_P\) to form the training set
\(T = \{(\pi_P^1, \CostStd(\pi_P^1)), (\pi_P^2, \CostStd(\pi_P^2)), \ldots, (\pi_P^m, \CostStd(\pi_P^m))\}\).
Having such training set allows us to find a vector \(\flatten{W_P}\)
that minimizes the mean square error
\begin{align*}
\frac{1}{m} \sum_{(\pi_P, \CostStd(\pi_P)) \in T} (\CostProxy(\pi_P, W_P) - \CostStd(\pi_P))^2
\end{align*}
by linear regression.

Minimizing the mean square error directly may lead to overfitting to the training set,
especially in problems whose signature is relatively large in comparison to the size of the training set.
\todo[author=Filip]{Justify along the following lines: Feature space dimension increases quadratically with signature size. Example: with 1000 samples and ... features we can fit exactly. 3 points in 3d etc., 1000 points in 1000d}
To improve generalization,
we use the Lasso regression algorithm \cite{Tibshirani1996} instead of standard linear regression.
%\todo[author=Filip]{I think we should use RidgeCV or ElasticNetCV instead because they minimize L2 norm, while lasso only minimizes L1 norm.
%Martin: also worth trying out experimentally.}
We use cross-validation to set the value of the regularization hyperparameter.\footnote{See the model \texttt{LassoCV} in the machine learning library scikit-learn \cite{scikit-learn}.}
%\todo[author=Filip]{Don't we need to use the same value of regularization parameter in order 
%for the preference matrix values to be comparable across problems?
%Martin: A good point. Something to experiment with in the future.}

Another reason to use the Lasso algorithm is that it performs regularization
by imposing a penalty on coefficients with large absolute value,
effectively shrinking the coefficients that correspond to symbol pairs
whose mutual order does not affect the \(\CostStd(\pi_P)\).
We can use this property to interpret the absolute value of preference value
as a measure of importance of a given symbol pair.

In the following sections we assume that the preference matrix \(W_P\) we find by Lasso regression
yields \(\CostProxy\) that approximates \(\CostStd\) well.

% \todo[inline,author=Filip]{Discuss: We assume that the overall cost is a sum of contributions of symbol pairs. This assumption may be too strong but we proceed with it anyway. Martin: already mentioned informally in the overview.}

\subsection{General preference matrix learning}
\label{sec:general-preference}

We proceed to cast the task of finding a good preference matrix \(W_P\)
for an arbitrary problem as a regression on feature vector representations of symbol pairs.
To accomplish this we need to be able to represent each pair of symbols by a feature vector
and to know target preference values for pairs of symbols in a training problem set.

\subsubsection{Target preference values}
\label{sec:target-preference-values}

For each problem \(P\) in the training problem set,
we find a problem preference matrix by the method outlined in \Cref{sect:preferences}.
The target value of an arbitrary pair of symbols \(s_i, s_j\) in \(P\) is \({W_P}_{i,j}\).

\subsubsection{Symbol pair embedding} \label{sect:embeddings}

We represent each symbol by a numeric \emph{feature vector} that consists of the following components:
symbol arity, % arity
the number of symbol occurrences in the problem, % usageCnt
the number of clauses in the problem that contain at least one occurrence of the symbol, % unitUsageCnt
% Filip: I have deduced the meaning of usageCnt, unitUsageCnt and inUnit
% by manual inspection of a few problems.
% Witness: PUZ056-1.p: predicate 'fits', function 'pin'
an indicator of occurrence in a conjecture clause, % inGoal
an indicator of occurrence in a unit clause, % inUnit
% Witness: PUZ001-1.p
an indicator of being produced during clausification. % introduced
% \todo[author=Filip]{I removed the dedicated "skolem" indicator because it is not included in the experiment because "intoduced" suffices. Is this okay?}
This choice of symbol features is motivated by the fact that they
are readily available in %the \gls*{atp} 
\gls*{vampire}
and that they suffice as a basis for common precedence generation schemes,
such as the \texttt{invfreq} scheme.
\newcommand{\fv}{\mathit{fv}}
We denote the feature vector corresponding to symbol \(s\) as \(\fv(s)\).

We represent a pair of symbols \(s, t\)
by the concatenation of their feature vectors \([\fv(s), \fv(t)]\).

\subsubsection{Training data}

The general preference regressor is trained on samples of the following structure:

\begin{itemize}
	\item the input: \([\fv(s_i), \fv(s_j)]\) -- the embedding of a symbol pair \(s_i, s_j\) in problem \(P\),
	\item the target: \({W_P}_{i,j}\) -- an element of the preference matrix we learned for problem \(P\)
	corresponding to the symbol pair \((s_i,s_j)\).
\end{itemize}
We sample problem \(P\) from the training problem set with uniform probability.
%\todo[author=Filip]{Actually we immediately exclude problems whose preference matrix is all-zero.
%Martin: I think it is fine not to say this and still do it that way.}

Thanks to how \(W_P\) is constructed (see \Cref{sect:preferences}),
preference values close to 0 are associated with symbol pairs whose mutual order has little effect
on the outcome of the proof search.
To focus the training on the symbol pairs whose order does matter,
we weight the samples by the absolute value of the target.
%
Preliminary experiments have shown that using sample weighting
improves the performance of the resulting model.

% Result data file: sftp://cluster.ciirc.cvut.cz/home/bartefil/git/vampire-ml/map-reduce/predicate-100-1000/2020-05-08_20-49-14/reduce/fit_cv_results.csv
% Rows where params is {} or {weighted: False} show impact of sample weighting (both problem and symbol pair).
% Disabling weighting leads to collapse of fitted coefficients, as demonstrated in feature_weights.csv.

% \todo[inline,author=Filip]{Will we use problem weighting after all? It is more difficult to justify...}

\newcommand{\GeneralRegressor}{M}
We denote the trained model as \(\GeneralRegressor\)
and its prediction of preference value of the symbol pair \(s_i, s_j\)
as \(\GeneralRegressor([\fv(s_i), \fv(s_j)])\).

\subsection{Precedence construction} \label{sect:construction}

When presented a new problem \(P = (\symbols, \clauses)\), we propose a symbol precedence by taking the following steps:
\begin{enumerate}
	\item Estimate a preference matrix \(\widehat{W_P}\).
	\item Construct a precedence \(\widehat{\pi_P}\) that approximately minimizes
	\(\CostProxy(\widehat{\pi_P}, \widehat{W_P})\).
\end{enumerate}

\subsubsection{Preference matrix construction}

To construct a preference matrix \(\widehat{W_P}\) for a new problem \(P\),
we evaluate the general preference regressor
on the feature vectors of all symbol pairs in \(P\).
More specifically,
\begin{align*}
\widehat{W_P}_{i, j} = M([\fv(s_i), \fv(s_j)])
\end{align*}
for all \(s_i, s_j \in \symbols\).

At this moment, one can use \(\widehat{W_P}\) to estimate the cost of an arbitrary symbol precedence.

\subsubsection{Precedence construction from preference matrix} \label{sect:ltot}

The remaining task is, given a preference matrix \(\widehat{W_P}\), to find a precedence \(\widehat{\pi_P}\)
that minimizes \(\CostProxy(\widehat{\pi_P}, \widehat{W_P})\).
Since this task is NP-hard in general \cite{Cohen2011},
we rely in this work on a greedy 2-approximation algorithm
proposed by a \citet{Cohen2011}.
%\todo[author=Filip]{Discuss: Is the algorithm acceptable despite the fact that it does not guarantee to find the optimum?
%	Martin: tak napisme, ze jinak musime projit $n!$ moznosti...
%	Martin2: v dalsim rozsireni bys mohl vyzkouset a zmerit hillclimb. Da se startovat ze seedu od Cohena,
%	nebo i z nahodnych seedu opakovane.}
The rest of this section provides a brief description of the algorithm.

\newcommand{\SymbolsAvail}{\symbols_{\mathit{avail}}}
\newcommand{\potential}{c}

The algorithm maintains a partially constructed symbol precedence \(\mathbf{p} \in \nat^*\)
(a finite sequence over \(\nat\); initially empty),
a set of available symbols \(\SymbolsAvail \subseteq \symbols\) (initially the whole \(\symbols\))
and a \emph{potential value} for each of the symbols \(\potential: \SymbolsAvail \to \re\).
The potential value of a symbol corresponds to the relative increase in proxy cost
associated with selecting the symbol as the next to append to the partial precedence:
\begin{align*}
\potential(s_i) = \sum_{s_j \in \SymbolsAvail} \widehat{W_P}_{i, j} - \sum_{s_j \in \SymbolsAvail} \widehat{W_P}_{j, i}
\end{align*}

In each iteration, a symbol \(s_i\) with the smallest potential is selected from \(\SymbolsAvail\).
This symbol is removed from \(\SymbolsAvail\)
and its index \(i\) is appended to the partial precedence \(\mathbf{p}\).
The potentials of the remaining symbols in \(\SymbolsAvail\) are updated.
This process is repeated until all symbols have been selected,
yielding the final \(\mathbf{p}\) as \(\widehat{\pi_P}\).

% Implementation: https://github.com/filipbartek/vampire-ml/blob/master/vampire_ml/precedence.py

% Dneska uz ne:
% \todo[inline,author=Filip]{Prove: If pref over embeddings is a sum of fl(l) and fr(r), then we can find optimum pi in linear time. This holds namely if pref is linear.}
% \todo[inline,author=Filip]{Prove: If pref(l, r) is a sum of fl(l) and fr(r), then LtOT yields the optimum pi.}

\section{Evaluation}
\label{sec:evaluation}

\subsection{Setup}

Since the simplification orderings under consideration (\acrshort*{lpo} and \acrshort*{kbo})
never use the symbol precedence to compare a predicate symbol with a function symbol,
we can break down the symbol precedence  into a predicate precedence and a function precedence.
For the sake of simplicity and easier interpretation of the results,
we restrict our attention in this evaluation to predicate precedences.
Function symbols are ordered by the \texttt{invfreq} scheme.

%\todo[inline,author=Filip]{Discuss more.
%	Possible generalizations to combination of predicate and function precedences:
%	
%	- Learn predicate precedence in the context of frequency heuristic for function precedences. (current experiment)
%	
%	- Learn predicate precedence in the context of random function precedences.
%	
%	- Iterate: Learn predicate precedence in the context of function precedences from previous training epoch.
%	
%	- Train predicate and function preference regressor simultaneously with a shared underlying GNN.}

\newcommand{\ProblemsTrain}{\mathcal{P}_{\mathit{train}}}
\newcommand{\ProblemsTest}{\mathcal{P}_{\mathit{test}}}

We use problems from the TPTP library v7.2.0 \cite{Sut17} for the evaluation.
Let \(\ProblemsTrain\) be the set of all \acrshort*{fol} and \acrshort*{cnf} problems in TPTP
with at most 200 predicate symbols such that at least 1 out of 24 random predicate precedences
leads to a successful proof search (\(|\ProblemsTrain| = 8217\)).
% problems/predicate-small-1024.txt & problems/predicate-solvable.txt & n_predicates <= 200
Let \(\ProblemsTest\) be the set of all \acrshort*{fol} and \acrshort*{cnf} problems in TPTP
with at most 1024 predicate symbols (\(|\ProblemsTest| = 15751\)).
% problems/predicate-small-1024.txt
In each of 5 evaluation iterations (splits),
we sample 1000 training problems from \(\ProblemsTrain\)
and 4108 test problems from \(\ProblemsTest\)
uniformly in a way that ensures that the sets do not overlap.
We repeat the experiment 5 times to evaluate the stability of the training.

On each training problem we run %the \gls{atp} 
Vampire with 100 uniformly random predicate precedences
and a strategy fixed up to the predicate precedence.\footnote{Time limit: 10 seconds, memory limit: 8192 MB, literal comparison mode: predicate, function symbol precedence: \texttt{invfreq}, saturation algorithm: discount, age-weight ratio: 1:10, AVATAR: disabled.}
% vampire --literal_comparison_mode predicate --symbol_precedence frequency --saturation_algorithm discount --age_weight_ratio 10 --avatar off --time_limit 10 --memory_limit 8192
Note that we use a customized version of Vampire to extract a symbol table from each of the problems.\footnote{\url{https://github.com/filipbartek/vampire/tree/926154f2}}
% `git reflog` shortens the commit hashes to a prefix of length 8.

After we fit a preference matrix on each of the training problems (see \Cref{sect:preferences}),
we create a batch of \(10^6\) symbol pair feature vectors with target values
to train the general preference regressor (see \Cref{sec:general-preference}).
We evaluate the trained model by running Vampire on the test problem set with predicate precedences
proposed by the trained model,
counting the number of successfully solved problems.

All the scripts used for the evaluation can be found at
\url{https://github.com/filipbartek/vampire-ml/tree/3f18886d}.
\todo[author=Filip]{Add a main launcher script and update the link.}

\subsection{Experimental results}

We trained the two types general preference regressors (see \Cref{sec:general-preference}):
\begin{itemize}
	\item Elastic-Net -- a linear regression model with L1 and L2 norm regularization; \\
	see \texttt{ElasticNetCV} in \citet{scikit-learn}
	\item Gradient Boosting regressor -- see \texttt{GradientBoostingRegressor} in \citet{scikit-learn}
\end{itemize}

We compared the performance of the regressors
with three baseline precedence generation schemes -- random precedence, best of 10 random precedences
and the \texttt{invfreq} scheme.
\Cref{table:results} shows the results of evaluation on 1000 problems
for 5 random choices of training and test problem set (splits).

\begin{table}[ht]
	\centering
	\begin{tabular}{l|rrrrr|r}
		Case & \multicolumn{6}{c}{Successes out of 1000 per split} \\
		& 0 & 1 & 2 & 3 & 4 & Mean \\
		\hline
		Best of 10 random & 514 & 499 & 509 & 511 & 476 & 501.8 \\
		\texttt{invfreq} & 494 & 472 & 480 & 481 & 452 & 475.8 \\
		Elastic-Net & 484 & 471 & 479 & 470 & 454 & 471.6 \\
		Gradient Boosting & 473 & 462 & 475 & 475 & 439 & 464.8 \\
		Elastic-Net without sample weighting & 475 & 454 & 465 & 459 & 453 & 461.2 \\
		Random & 454 & 455 & 457 & 456 & 430 & 450.4 \\
	\end{tabular}
	% Source: https://docs.google.com/spreadsheets/d/1HSsC7piUAtWt6uwA9SYOX5vmiF0Ab3Ae4FPyGsDALIg/edit?usp=sharing
	\caption{Experiment results}
	\label{table:results}
\end{table}

The case ``Elastic-Net without sample weighting'' shows the effect of
sampling the symbol pairs uniformly.
Inspection of the trained feature coefficients reveals that the fitting ends up
with an all-zero feature weight vector on splits 1 and 2,
signifying a complete failure to learn on these training sets.

Using Elastic-Net for general preference prediction on average nearly matches the performance
of Vampire with the \texttt{invfreq} precedence scheme.
While Elastic-Net performs significantly better than random precedence generator,
it still performs significantly worse than a generator
that, given a problem, tries 10 random precedences and chooses the best of these.
This suggests that there is space for improvement, possibly with a more sophisticated, non-linear model.
Plugging in a Gradient Boosting regressor does not show immediate improvement
so more elaborate feature extraction may be necessary.

%The current architecture allows using arbitrary symbol feature vectors
%and can be modified to backpropagate loss gradients.
%This opens a way for using neural networks as feature extractors.

\subsection{Feature coefficients}

Since Elastic-Net is a linear regression model, we can easily inspect the coefficients it assigns
to the input features (see \Cref{sect:embeddings}).
In each of the five splits, the final coefficients of the three indicator features are 0.
\Cref{table:feature-weights} shows the fitted non-zero coefficients of the remaining features.
The coefficients were scaled so that their absolute values sum up to 1 and zero-valued coefficients omitted.
Note that scaling the coefficients by a constant does not affect
the precedence constructed using the greedy algorithm presented in \Cref{sect:ltot}.

\begin{table}[ht]
	\centering
	\begin{tabular}{l|rrr|rrr}
		Training set & \multicolumn{3}{c}{Left symbol} & \multicolumn{3}{c}{Right symbol} \\
		& Arity & Frequency & Unit frequency & Arity & Frequency & Unit frequency \\
		\hline
		0 &     &-0.01&     &-0.98& 0.01&      \\
		1 &     &-0.48&     &     & 0.08& 0.44 \\
		2 &     &     &-0.64&     & 0.36&      \\
		3 & 0.88&-0.03& 0.01&     &-0.03& 0.05 \\
		4 &     &-0.62&     &     & 0.30& 0.07 \\
		\hline
		\(\ProblemsTrain\) &     &     &-0.57&     & 0.43&      \\
	\end{tabular}
	% Source: https://docs.google.com/spreadsheets/d/1HSsC7piUAtWt6uwA9SYOX5vmiF0Ab3Ae4FPyGsDALIg/edit?usp=sharing
	\caption{Elastic-Net feature coefficients after fitting on each of the 5 training sets of 1000 problems and on the whole \(\ProblemsTrain\).
		``Frequency'' is the number of occurrences of the symbol in the problem.
		``Unit frequency'' is the number of clauses in the problem that contain at least one occurrence of the symbol.}
	\label{table:feature-weights}
\end{table}

It is worth pointing out that the regressor fitted on the whole \(\ProblemsTrain\)
assigns a high preference value to symbol pairs \((s, t)\)
such that \(t\) has a higher frequency and unit frequency than \(s\).
Since unit frequency is positively correlated with frequency,
minimizing \(\CostProxy\) using this fitted regressor
is consistent with the \texttt{invfreq} precedence generating scheme
(ordering the symbols by frequency in descending order).

\section{Conclusion}

This paper is, to the best of our knowledge, a first attempt to use machine learning for proposing symbol precedences.
This appears to be a potentially highly rewarding task with the possibility of generating effectively unlimited
amount of training data. Nevertheless, the journey from evaluating the prover on random precedences 
to proposing a good precedence when presented with a new problem is not straightforward and several 

% In this work


% Future




\bibliographystyle{plainnat}
\bibliography{main}

\end{document}
