\input{preamble}

\title{Learning Precedences from Elementary \todo[author=Filip]{Should we call the features "primitive" instead of "elementary"?} Symbol Features\thanks{Supported by the ERC Consolidator grant AI4REASON no. 649043 under the EU-H2020 programme.}}
\titlerunning{Learning Precedences from Elementary Symbol Features}
\author{Filip B\'{a}rtek \and Martin Suda}
\authorrunning{B\'{a}rtek, Suda}
\institute{Czech Technical University in Prague, Czech Republic}

\begin{document}

\maketitle

\begin{abstract}
The choice of predicate symbol precedence can have a great impact on the success of proof search in saturation-based automated theorem provers. We explore the possibility of machine-learning a predicate symbol precedence proposal system based on elementary predicate symbol features. Training a linear model converges to ordering the symbols by the number of their occurrences.
\todo[author=Filip]{Update the abstract once the experimental results are finalized.}
\end{abstract}

\section{Introduction}

\Gls{preference-value} of an ordered pair of symbols \((P_0, P_1)\) expresses a penalty associated with ordering the symbol \(P_0\) before the symbol \(P_1\).
Positive value means that ordering \(P_0\) before \(P_1\) is associated with low probability of success.
Negative value means that ordering \(P_0\) before \(P_1\) is associated with high probability of success.
Value close to zero means that the order of symbols \(P_0\) and \(P_1\) has no effect on the probability of success.

\Gls{preference-matrix} is a square matrix populated with preference values of all the pairs of symbols.

For a given symbol precedence,
\gls{symbol-order-matrix} \(C\) is a boolean square matrix of size \(n \times n\) where \(n\) is the number of symbols.
\(C_{i, j}\) is true if and only if symbol \(P_i\) precedes symbol \(P_j\) in the precedence.

\section{Architecture}

\subsection{Prediction}

For a given problem, symbol precedence prediction consists of two steps:

\begin{enumerate}
	\item Preference matrix estimation
	\item Precedence construction from preference matrix
\end{enumerate}

\subsubsection{Preference matrix estimation}

Given a problem, we compute an embedding for each pair of symbols.
An embedding of a pair of symbols is a concatenation of the embeddings of the two symbols.
The embedding of a symbol consists of the following features:

\begin{itemize}
	\item Arity
	\item Usage count
	\item Unit usage count
	\item In goal
	\item In unit
	\item Skolem
\end{itemize}

The preference value regressor predicts a preference value for each symbol pair embedding.
We poll the preference value regressor for each pair of symbols of the input problem,
storing the preference values in a preference matrix.

\subsubsection{Precedence construction}

Given a preference matrix, we construct a symbol ordering
that approximately maximizes cumulative preference
using a greedy algorithm presented in LtOT.

\subsection{Training}

The training of preference value predictor consists of two stages:

\begin{enumerate}
	\item Problem-wise target preference matrix estimation
	\item Preference value regressor fitting
\end{enumerate}

\subsubsection{Problem-wise target preference matrix estimation}

For each problem,
we train a linear predictor
that predicts target score from \glspl{symbol-order-matrix}.
The coefficients of the trained predictor are the target preference values of the symbol pairs for this problem.

\subsubsection{Preference value regressor fitting}

As outlined in section ...,
the preference value regressor estimates a preference value for each symbol pair embedding.
It can be trained using any 

\section{Evaluation}

\section{Conclusion}

\glsaddall
\printglossaries

\end{document}
