\input{preamble}

\title{Learning Precedences from Simple Symbol Features\thanks{Supported by the ERC Consolidator grant AI4REASON no. 649043 under the EU-H2020 programme and the Czech Science Foundataion project 20-06390Y.}}
\titlerunning{Learning Precedences from Elementary Symbol Features}
\author{Filip B\'{a}rtek \and Martin Suda}
\authorrunning{B\'{a}rtek, Suda}
\institute{Czech Technical University in Prague, Czech Republic}

\begin{document}

\maketitle

\begin{abstract}
A simplification ordering, typically specified by a symbol precedence,
is one of the key parameters of the superposition calculus, contributing
to shaping the search space navigated by a saturation-based automated theorem prover.
Thus the choice of a precedence can have a great impact on the prover's performance.
In this work, we design a system for proposing favourable predicate symbol precedences.
The system relies on machine learning to extract the information from
past runs of a theorem prover over a set of problems and random precedences.
Moreover, it uses a small set of simple human-engineered symbol features as the sole
basis for discriminating the symbols. This allows for a direct comparison
with precedence construction heuristics designed by prover developers.
% Martin: let's add this when we have the results finalised
% Training a linear model converges to ordering the symbols by the number of their occurrences.
\todo[author=Filip]{Update the abstract once the experimental results are finalized.}
\end{abstract}

\section{Introduction}

\Gls{preference-value} of an ordered pair of symbols \((P_0, P_1)\) expresses a penalty associated with ordering the symbol \(P_0\) before the symbol \(P_1\).
Positive value means that ordering \(P_0\) before \(P_1\) is associated with low probability of success.
Negative value means that ordering \(P_0\) before \(P_1\) is associated with high probability of success.
Value close to zero means that the order of symbols \(P_0\) and \(P_1\) has no effect on the probability of success.

\Gls{preference-matrix} is a square matrix populated with preference values of all the pairs of symbols.

For a given symbol precedence,
\gls{symbol-order-matrix} \(C\) is a boolean square matrix of size \(n \times n\) where \(n\) is the number of symbols.
\(C_{i, j}\) is true if and only if symbol \(P_i\) precedes symbol \(P_j\) in the precedence.

\section{Architecture}

\subsection{Prediction}

For a given problem, symbol precedence prediction consists of two steps:

\begin{enumerate}
	\item Preference matrix estimation
	\item Precedence construction from preference matrix
\end{enumerate}

\todo[inline,author=Filip]{Explain why we chose this decomposition.}

\subsubsection{Preference matrix estimation}

Given a problem, we compute an embedding for each pair of symbols.
An embedding of a pair of symbols is a concatenation of the embeddings of the two symbols.
The embedding of a symbol consists of the following features:

\begin{itemize}
	\item Arity
	\item Usage count
	\item Unit usage count
	\item In goal
	\item In unit
	\item Skolem
\end{itemize}

The preference value regressor predicts a preference value for each symbol pair embedding.
We poll the preference value regressor for each pair of symbols of the input problem,
storing the preference values in a preference matrix.

In the special case of linear regressor, we can omit the construction of preference matrix
and order the symbols using the weight vector of the regressor,
as shown below.

\subsubsection{Precedence construction}

Given a preference matrix, we construct a symbol ordering
that approximately minimizes cumulative preference value
using a greedy algorithm presented in LtOT.

More precisely, the algorithm approximately minimizes the following value:

\begin{align*}
s(\pi) = \sum_{\pi(l) < \pi(r)} s(l, r) - s(r, l)
\end{align*}

\todo[inline,author=Filip]{Justify lemma: argmin sum s(l,r) - s(r,l) = argmin sum s(l,r)}

\paragraph{Linear regressor}

Note that if \(s\) is constructed using a linear regressor,
then there is a function \(\bar{s}\) that satisfies the following property:

\begin{align*}
s(l, r) - s(r, l) = \bar{s}(l) - \bar{s}(r)
\end{align*}

Then:

\begin{align*}
s(\pi) = \sum_{\pi(l) < \pi(r)} \bar{s}(l) - \bar{s}(r)
= \sum_{\pi(l) < \pi(r)} \bar{s}(l) - \sum_{\pi(l) < \pi(r)} \bar{s}(r)
= \sum_{x} \bar{s}(x) (\bar{\pi}(x) - \pi(x))
= \sum_{x} \bar{s}(x) (n + 1 - 2 \pi(x))
\end{align*}

It is easy to see that the \(\pi\) that minimizes \(s(\pi)\) orders the symbols in ascending order by their \(\bar{s}\) values.
If we know the \(\bar{s}\) value of each symbol without computing the preference matrix,
we need not compute the preference matrix.
This happens to be the case with linear regressor.

\subsection{Training}

The training of preference value predictor consists of two stages:

\begin{enumerate}
	\item Problem-wise target preference matrix estimation
	\item Preference value regressor fitting
\end{enumerate}

\subsubsection{Problem-wise target preference matrix estimation}

For each problem,
we train a linear predictor
that predicts target score from \glspl{symbol-order-matrix}.
The coefficients of the trained predictor are the target preference values of the symbol pairs for this problem.

\subsubsection{Preference value regressor fitting}

As outlined in section ...,
the preference value regressor estimates a preference value for each symbol pair embedding.
It can be trained using any 

\section{Evaluation}

\section{Conclusion}

\glsaddall
\printglossaries

\end{document}
