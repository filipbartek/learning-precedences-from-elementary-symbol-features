\input{preamble}

\title{Learning Precedences from Simple Symbol Features\thanks{Supported by the ERC Consolidator grant AI4REASON no. 649043 under the EU-H2020 programme and the Czech Science Foundataion project 20-06390Y.}}
\titlerunning{Learning Precedences from Elementary Symbol Features}
\author{Filip B\'{a}rtek \and Martin Suda}
\authorrunning{B\'{a}rtek, Suda}
\institute{Czech Technical University in Prague, Czech Republic}

\begin{document}

\maketitle

\begin{abstract}
A simplification ordering, typically specified by a symbol precedence,
is one of the key parameters of the superposition calculus, contributing
to shaping the search space navigated by a saturation-based \acrlong{atp}.
Thus the choice of a precedence can have a great impact on the prover's performance.
In this work, we design a system for proposing favourable predicate symbol precedences.
The system relies on machine learning to extract the information from
past runs of a theorem prover over a set of problems and random precedences.
Moreover, it uses a small set of simple human-engineered symbol features as the sole
basis for discriminating the symbols. This allows for a direct comparison
with precedence construction heuristics designed by prover developers.
\todo[author=Martin,inline]{Let's add this when we have the results finalized:
Training a linear model converges to ordering the symbols by the number of their occurrences.}
\end{abstract}

\section{Introduction}

The proof search on a given problem using superposition calculus is parameterized by two orderings:
\todo[author=Martin,inline]{In the official terminology, there just one ordering. Symbol precedence is just one
parameter of certain family of orderings.}

\begin{itemize}
	\item Predicate precedence -- an ordering on predicate symbols of the problem
	\item Function precedence -- an ordering on function symbols of the problem
\end{itemize}

In this article we limit ourselves to discussion of predicate symbol precedences for the sake of simplicity.
We fix function precedences to use the frequency heuristic.

Given a problem, we look for a predicate \gls{precedence} that lead to successful and quick proof search.

\Gls{preference-value} of an ordered pair of symbols \((P_0, P_1)\) expresses a penalty associated with ordering the symbol \(P_0\) before the symbol \(P_1\).
Positive value means that ordering \(P_0\) before \(P_1\) is associated with low probability of success.
Negative value means that ordering \(P_0\) before \(P_1\) is associated with high probability of success.
Value close to zero means that the order of symbols \(P_0\) and \(P_1\) has no effect on the probability of success.

\Gls{preference-matrix} is a square matrix populated with preference values of all the pairs of symbols.

For a given symbol precedence,
\gls{symbol-order-matrix} \(C\) is a boolean square matrix of size \(n \times n\) where \(n\) is the number of symbols.
\(C_{i, j}\) is true if and only if symbol \(P_i\) precedes symbol \(P_j\) in the precedence.

\section{Architecture}

\subsection{Basic assumptions}

For the discussion that follows in this section,
we assume a fixed \gls{fol} \gls{atp} that uses superposition calculus.
While the practical experiments described in \autoref{sec:evaluation} use the \gls{atp} Vampire \cite{?},
the model architecture does not assume a particular \gls{atp}
and is compatible with any superposition-based\todo[author=Filip]{Is this term admissible?}\todo[author=Martin,inline]{I think so.
What we rely on here, is the use of a simplification ordering, such as KBO or LPO parametrized by a symbol precedence.}
\gls{atp} such as Vampire or E.

Furthermore, we fix the problem-agnostic configuration of the chosen \gls{atp}.
The configuration may specify for example a literal comparison mode, a saturation algorithm
and a time limit.
We assume that a finite time limit is used because very long proof searches are common in practice\cite{?}
and we prefer to handle such searches gracefully.
\todo[author=Filip]{Improve the justification.}
\todo[inline,author=Filip]{Is the prover guaranteed to converge on any solvable problem irrespective of the precedence? If it is, then we could train on time-unlimited proof searches.}
\todo[inline,author=Martin]{In theory yes, for a complete strategy (and we are using such), but in practice this could take zillions of years. We would never finish before a deadline ;))}

For sake of simplicity,
we will describe a system that only proposes predicate precedences.
\todo[inline,author=Filip]{Discuss more.
Possible generalizations to combination of predicate and function precedences:

- Learn predicate precedence in the context of frequency heuristic for function precedences. (current experiment)

- Learn predicate precedence in the context of random function precedences.

- Iterate: Learn predicate precedence in the context of function precedences from previous training epoch.

- Train predicate and function preference regressor simultaneously with a shared underlying GNN.}

\subsection{Main goals}

We strive to create a system that satisfies the following properties:

\begin{itemize}
	\item When presented with an arbitrary \gls{fol} problem,
	the system proposes a predicate precedence for this problem
	that maximizes the expected chance of the problem being solved
	within the allocated time.
	
	\todo[inline,author=Martin]{Maybe simply minimize the expected (abstract) solving time (i.e. measured in the number of iterations of the main loop!).}
	
	\item The system is trained on a collection of executions of the chosen prover
	on a training set of \gls{fol} problems
	with uniformly random predicate precedences.
\end{itemize}

\subsection{Valuation of precedences}

Based on the assumption that a short proof search is more likely to succeed in a limited time\cite{?},
we train the system to prefer precedences that lead to a short proof search.
This allows us to use easy problems for training, facilitating the collection of training data.

We define the loss value of precedence \(\pi\) according to the outcome of the proof search
configured to use this precedence:

\begin{itemize}
	\item If the proof search terminates within the allocated time,
	\(\base_loss(\pi)\) is the number iterations of saturation loop started during the proof search.
	\item If the proof search times out, \(\base_loss(\pi)\) is the maximum number
	of saturation loop iterations encountered in successful proof searches on this problem
	with uniformly random precedences.
\end{itemize}

We further transform the loss values by the following operations:

\begin{enumerate}
	\item Log-scale: Improvement by a constant factor is a cross-problem \todo[inline,author=Martin]{?} captures the intuition of the same level of improvement across problems.
	\item Standardization: For each problem,
	we apply an affine transformation so that the loss values have the mean 0 and standard deviation 1.
	the loss values so that the loss values are comparable across problems.
\end{enumerate}

We further log-scale the loss values
and standardize\cite{?} the loss values to make them comparable across problems.

\subsection{Reduction to preference learning}

Let's \todo[inline,author=Martin]{Vsechny zkraceniny jsou povazovany za prilis neformalni.} suppose we have a set of \(m\) problems
and that we run the prover on each of them with \(n\) uniformly random predicate precedences.
How can we use the resulting data to learn a precedence proposal system?

\subsection{Discussion}

Let \(\pi\) be a predicate precedence on a given \gls{fol} problem.
Let \(\loss(\pi)\) be a real number that corresponds to the performance of the chosen \gls{atp}
on the given \gls{fol} problem when using the predicate precedence \(\pi\).

Let the \gls{atp} Prover run on problem Problem with a time limit of 10 seconds.


For example, \(\loss(\pi)\) may be defined as the number of saturation loop iterations in case the prover

We strive to design and train a system that, when presented with any \gls{fol} problem,
proposes a predicate precedence for this problem with low expected \(loss\).
We want to train this system on a collection of executions of an \gls{atp}\footnote{We use the \gls{atp} Vampire in out experiments.} with random precedences on a training set of problems.

We break the task of generating a good precedence down to two stages:
\todo[author=Filip]{Why do we do that? \citet{Cohen2011} assumes that the pairwise preference is the input.}

\begin{enumerate}
	\item Estimate a pairwise loss function \(\loss_2\).
	\item Generate a permutation that maximizes the cumulative preference value.
\end{enumerate}

Notation:

\(\pi(i)\) is the \(i\)-th symbol under precedence \(\pi\).

\(\inv{\pi}(P)\) is the position of symbol \(P\) in precedence \(\pi\).

We work under the assumption that the overall performance of precedence \(\pi\) decomposes into a sum of contributions of ordered pairs of symbols:

\begin{align*}
\exists \loss_2 : \predicates \times \predicates \rightarrow \re . \loss(\pi) = \sum_{P, Q: \inv{\pi}(P) < \inv{\pi}(Q)} \loss_2(P, Q)
\end{align*}

Working under this assumption allows us to use \(\loss_2\) as a proxy for minimizing \(\loss\).
Note that each of the precedence heuristics implemented in Vampire can be expressed as minimizers of certain \(\loss_2\):

...



\section{Architecture}

\subsection{Prediction}

For a given problem, symbol precedence prediction consists of two steps:

\begin{enumerate}
	\item Preference matrix estimation
	\item Precedence construction from preference matrix
\end{enumerate}

\subsubsection{Preference matrix estimation}

Given a problem, we compute an embedding for each pair of symbols.
An embedding of a pair of symbols is a concatenation of the embeddings of the two symbols.
The embedding of a symbol consists of the following features:

\begin{itemize}
	\item Arity
	\item Usage count
	\item Unit usage count
	\item In goal
	\item In unit
	\item Skolem
\end{itemize}

The preference value regressor predicts a preference value for each symbol pair embedding.
We poll the preference value regressor for each pair of symbols of the input problem,
storing the preference values in a preference matrix.

\subsubsection{Precedence construction}

Given a preference matrix, we construct a symbol ordering
that approximately maximizes cumulative preference
using a greedy algorithm presented in LtOT.

\subsection{Training}

The training of preference value predictor consists of two stages:

\begin{enumerate}
	\item Problem-wise target preference matrix estimation
	\item Preference value regressor fitting
\end{enumerate}

\subsubsection{Problem-wise target preference matrix estimation}

For each problem,
we train a linear predictor
that predicts target score from \glspl{symbol-order-matrix}.
The coefficients of the trained predictor are the target preference values of the symbol pairs for this problem.

\subsubsection{Preference value regressor fitting}

As outlined in section ...,
the preference value regressor estimates a preference value for each symbol pair embedding.
It can be trained using any 

\section{Evaluation}
\label{sec:evaluation}

\section{Conclusion}

\glsaddall
\printglossaries

\bibliographystyle{plainnat}
\bibliography{main}

\end{document}
