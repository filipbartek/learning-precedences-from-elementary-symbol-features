\input{preamble}

\title{Learning Precedences from Simple Symbol Features\thanks{Supported by the ERC Consolidator grant AI4REASON no. 649043 under the EU-H2020 programme and the Czech Science Foundataion project 20-06390Y.}}
\titlerunning{Learning Precedences from Elementary Symbol Features}
\author{Filip B\'{a}rtek \and Martin Suda}
\authorrunning{B\'{a}rtek, Suda}
\institute{Czech Technical University in Prague, Czech Republic}

\begin{document}

\maketitle

\begin{abstract}
A simplification ordering, typically specified by a symbol precedence,
is one of the key parameters of the superposition calculus, contributing
to shaping the search space navigated by a saturation-based \acrlong{atp}.
Thus the choice of a precedence can have a great impact on the prover's performance.
In this work, we design a system for proposing favourable predicate symbol precedences.
The system relies on machine learning to extract the information from
past runs of a theorem prover over a set of problems and random precedences.
Moreover, it uses a small set of simple human-engineered symbol features as the sole
basis for discriminating the symbols. This allows for a direct comparison
with precedence construction heuristics designed by prover developers.
\todo[author=Martin,inline]{Let's add this when we have the results finalized:
Training a linear model converges to ordering the symbols by the number of their occurrences.}
\end{abstract}

\section{Introduction}

The most successful \gls{fol} \glspl{atp} are based on superposition calculus,
which is parameterized by simplification term ordering.
Each of the two most commonly used simplification orderings, \gls{kbo} and \gls{lpo},
is in turn parameterized by a pair of symbol precedences:
predicate precedence and function precedence.
Each of these precedences is a permutation of the respective symbols of the problem under consideration.

In this text we limit ourselves to discussion of predicate symbol precedences for the sake of simplicity.
We fix function precedences to use the frequency heuristic.

Given a problem, we look for a predicate \gls{precedence} that leads to a successful and quick proof search.

\Gls{preference-value} of an ordered pair of symbols \((P_0, P_1)\) expresses a penalty associated with ordering the symbol \(P_0\) before the symbol \(P_1\).
Positive value means that ordering \(P_0\) before \(P_1\) is associated with low probability of success.
Negative value means that ordering \(P_0\) before \(P_1\) is associated with high probability of success.
Value close to zero means that the order of symbols \(P_0\) and \(P_1\) has no effect on the probability of success.

\Gls{preference-matrix} is a square matrix populated with preference values of all the pairs of symbols.

For a given symbol precedence,
\gls{order-matrix} \(C\) is a boolean square matrix of size \(n \times n\) where \(n\) is the number of symbols.
\(C_{i, j}\) is true if and only if symbol \(P_i\) precedes symbol \(P_j\) in the precedence.

\section{Architecture}

\subsection{Basic assumptions}

For the discussion that follows in this section,
we assume a fixed \gls{fol} \gls{atp} that uses superposition calculus parameterized by symbol precedence.
While the practical experiments described in \autoref{sec:evaluation} use the \gls{atp} Vampire \cite{?},
the model architecture does not assume a particular \gls{atp}
and is compatible with any superposition-based \gls{atp} such as Vampire or E.

Furthermore, we fix the problem-agnostic strategy parameters of the chosen \gls{atp}.
The parameters may specify for example a literal comparison mode, a saturation algorithm
and a time limit.
We specifically assume that a finite time limit is used
because some proof searches may be too long for practical use\cite{?}
and we need to handle such searches gracefully.
% Note that an ATP that is complete is guaranteed to halt on a solvable problem.
% However, the solving time may vary greatly across the precedences.

For sake of simplicity,
we will describe a system that only proposes predicate precedences.
\todo[inline,author=Filip]{Discuss more.
Possible generalizations to combination of predicate and function precedences:

- Learn predicate precedence in the context of frequency heuristic for function precedences. (current experiment)

- Learn predicate precedence in the context of random function precedences.

- Iterate: Learn predicate precedence in the context of function precedences from previous training epoch.

- Train predicate and function preference regressor simultaneously with a shared underlying GNN.}

\subsection{Main goals}

We strive to create a system that satisfies the following properties:

\begin{itemize}
	\item When presented with an arbitrary \gls{fol} problem,
	the system proposes a predicate precedence for this problem
	that maximizes the expected chance of the problem being solved
	within the allocated time.
	
	\todo[inline,author=Martin]{Maybe simply minimize the expected (abstract) solving time (i.e. measured in the number of iterations of the main loop!).}
	
	\item The system is trained on a collection of executions of the chosen prover
	on a training set of \gls{fol} problems
	with uniformly random predicate precedences.
\end{itemize}

\todo[inline,author=Filip]{Should we mention the following side goal? "Make it possible to backpropagate gradient to the input features for future GNN training."}

\subsection{Valuation of precedences}

Based on the assumption that a short proof search is more likely to succeed in a limited time\cite{?},
we train the system to prefer precedences that lead to a short proof search.
This allows us to use easy problems for training, facilitating the collection of training data.

We define the base loss
\todo[author=Martin]{The term "loss" bears unnecessary ML connotations so this may be confusing.}
\todo[author=Filip]{Alternatives: cost, penalty, "value", "objective value"}
value of precedence \(\pi\) on a given problem
according to the outcome of the proof search configured to use this precedence:

\begin{itemize}
	\item If the proof search terminates within the allocated time,
	\(\base_loss(\pi)\) is the number of iterations of the saturation loop
	started during the proof search.
	\item If the proof search times out, \(\base_loss(\pi)\) is the maximum number
	of saturation loop iterations encountered in successful proof searches on this problem.
\end{itemize}

We further transform the loss values by the following operations:

\begin{enumerate}
	\item Log-scale: Multiplying the number of saturation iterations by a constant factor
	should have the same impact on the effective loss value
	irrespective of the problem and its particular distribution of saturation loop iteration counts.
	\item Standardization\cite{?}: For each problem,
	we apply an affine transformation so that the resulting loss values
	have the mean 0 and standard deviation 1.
	This ensures that the values are comparable across problems.
\end{enumerate}

Let \(\loss(\pi)\) denote the resulting loss value of permutation \(\pi\)
after scaling and standardization.

\subsection{Reduction to preference learning}

Let \(p \neq q\) be two predicates in a given \gls{problem}.
Each precedence \(\pi\) orders the predicates in some order --
either \(\inv{\pi}(p) < \inv{\pi}(q)\) or \(\inv{\pi}(p) > \inv{\pi}(q)\).
Ordering of some pairs of symbols can have a great impact on the speed of the proof search.
For example, prioritizing inferences on clauses with predicate symbols
introduced in Tseitin transformation
may lead to an exponential expansion of the affected clauses.

Let the pairwise preference function \(\pref : \Sigma_P \times \Sigma_P \rightarrow \re\)
assign a preference value to each pair of predicate symbols:
a high value of \(\pref(p, q)\) corresponds to a long expected proof search time using a permutation
that orders \(p\) before \(q\).
\todo[author=Filip]{Is it ok to use the name "preference" for a function that is to be minimized, thus actually a loss or a cost? I adopted it from LtOT terminology. Alternative: pairwise cost.}
Furthermore, let the absolute value of \(\pref(p, q)\)
correspond to the expected magnitude of impact of the mutual order of \(p\) and \(q\).
Namely, \(\pref(p, q) = 0\) means that the mutual order of \(p\) and \(q\)
has no impact on the length of the proof search.

We proceed to relax the general task of finding a good precedence
to the task of finding a good pairwise preference function.
Once we have such function, we can construct a precedence that approximately minimizes the cumulative preference value:

\begin{align*}
\loss_{proxy}(\pi) = \sum_{p, q: \inv{\pi}(p) < \inv{\pi}(q)} \pref(p, q)
\end{align*}

Note that such preference function may be trained by conventional machine learning techniques.

One possibility is to encode a precedence as an \gls{order-matrix}.

\subsection{Discussion}

Let \(\pi\) be a predicate precedence on a given \gls{fol} problem.
Let \(\loss(\pi)\) be a real number that corresponds to the performance of the chosen \gls{atp}
on the given \gls{fol} problem when using the predicate precedence \(\pi\).

Let the \gls{atp} Prover run on problem Problem with a time limit of 10 seconds.


For example, \(\loss(\pi)\) may be defined as the number of saturation loop iterations in case the prover

We strive to design and train a system that, when presented with any \gls{fol} problem,
proposes a predicate precedence for this problem with low expected \(loss\).
We want to train this system on a collection of executions of an \gls{atp}\footnote{We use the \gls{atp} Vampire in out experiments.} with random precedences on a training set of problems.

We break the task of generating a good precedence down to two stages:
\todo[author=Filip]{Why do we do that? \citet{Cohen2011} assumes that the pairwise preference is the input.}

\begin{enumerate}
	\item Estimate a pairwise loss function \(\loss_2\).
	\item Generate a permutation that maximizes the cumulative preference value.
\end{enumerate}

Notation:

\(\pi(i)\) is the \(i\)-th symbol under precedence \(\pi\).

\(\inv{\pi}(P)\) is the position of symbol \(P\) in precedence \(\pi\).

We work under the assumption that the overall performance of precedence \(\pi\) decomposes into a sum of contributions of ordered pairs of symbols:

\begin{align*}
\exists \loss_2 : \predicates \times \predicates \rightarrow \re . \loss(\pi) = \sum_{P, Q: \inv{\pi}(P) < \inv{\pi}(Q)} \loss_2(P, Q)
\end{align*}

Working under this assumption allows us to use \(\loss_2\) as a proxy for minimizing \(\loss\).
Note that each of the precedence heuristics implemented in Vampire can be expressed as minimizers of certain \(\loss_2\):

...



\section{Architecture}

\subsection{Prediction}

For a given problem, symbol precedence prediction consists of two steps:

\begin{enumerate}
	\item Preference matrix estimation
	\item Precedence construction from preference matrix
\end{enumerate}

\subsubsection{Preference matrix estimation}

Given a problem, we compute an embedding for each pair of symbols.
An embedding of a pair of symbols is a concatenation of the embeddings of the two symbols.
The embedding of a symbol consists of the following features:

\begin{itemize}
	\item Arity
	\item Usage count
	\item Unit usage count
	\item In goal
	\item In unit
	\item Skolem
\end{itemize}

The preference value regressor predicts a preference value for each symbol pair embedding.
We poll the preference value regressor for each pair of symbols of the input problem,
storing the preference values in a preference matrix.

\subsubsection{Precedence construction}

Given a preference matrix, we construct a symbol ordering
that approximately maximizes cumulative preference
using a greedy algorithm presented in LtOT.

\subsection{Training}

The training of preference value predictor consists of two stages:

\begin{enumerate}
	\item Problem-wise target preference matrix estimation
	\item Preference value regressor fitting
\end{enumerate}

\subsubsection{Problem-wise target preference matrix estimation}

For each problem,
we train a linear predictor
that predicts target score from \glspl{order-matrix}.
The coefficients of the trained predictor are the target preference values of the symbol pairs for this problem.

\subsubsection{Preference value regressor fitting}

As outlined in section ...,
the preference value regressor estimates a preference value for each symbol pair embedding.
It can be trained using any 

\section{Evaluation}
\label{sec:evaluation}

\section{Conclusion}

\glsaddall
\printglossaries

\bibliographystyle{plainnat}
\bibliography{main}

\end{document}
